{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3YSDiy0Up_C"
   },
   "source": [
    "# Evaluation: Precision & Recall\n",
    "## Using the evaluation metrics we have learned, we are going to compare how well some different types of classifiers perform on different evaluation metrics\n",
    "### We are going to use a dataset of written numbers which we can import from sklearn. Run the code below to do so. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fnUelxEmUp_D"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "X, y = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMgKR4YhUp_G"
   },
   "source": [
    "### Now take a look at the shapes of the X and y matricies \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "prseFXcoUp_H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,\n",
       "        18.,  18., 126., 136., 175.,  26., 166., 255., 247., 127.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        30.,  36.,  94., 154., 170., 253., 253., 253., 253., 253., 225.,\n",
       "       172., 253., 242., 195.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253., 253., 253.,\n",
       "       253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        18., 219., 253., 253., 253., 253., 253., 198., 182., 247., 241.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
       "       253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  11., 190., 253.,  70.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240.,\n",
       "       253., 253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 249., 253.,\n",
       "       249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253.,\n",
       "       250., 182.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 114.,\n",
       "       221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  23.,  66., 213., 253., 253., 253., 253., 198.,  81.,\n",
       "         2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n",
       "       253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  55.,\n",
       "       172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135.,\n",
       "       132.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XRz_ivu3Up_J"
   },
   "source": [
    "### Now, let's pick one entry and see what number is written. Use indexing to pick the 36000th digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIilw19uUp_J"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  16.,  29., 154.,\n",
       "       254., 243., 135.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  19.,  57., 166., 253., 253., 253., 254., 235.,  38.,  51.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0., 173., 253., 254., 253.,\n",
       "       187., 168., 169.,  93.,  41.,  48.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  76., 223., 253., 242., 116.,   6.,   0.,   0.,   0.,  98.,\n",
       "        78.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  26.,  92., 217., 254., 229., 102.,\n",
       "         0.,   0.,   0.,   0.,   7.,  19.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       150., 254., 253., 196.,   9.,   0.,   0.,   0.,   0.,   0.,  19.,\n",
       "       163., 120.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 154., 223., 254., 171.,  37.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  45., 216., 165.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  51.,\n",
       "       247., 253., 229.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       157., 253.,  90.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  13., 105., 254., 245., 176.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 105., 248., 150.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  89., 254.,\n",
       "       241., 169.,  13.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  51.,\n",
       "       254., 197.,  76.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  29., 235., 254., 184.,  25.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,  29., 235., 254., 134.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  98., 241.,\n",
       "       242.,  98.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  26., 123.,\n",
       "       241., 242.,  98.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  83., 230., 254., 114.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  13., 157., 254., 204., 114.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  19., 188., 253.,\n",
       "       152.,  38.,   0.,   0.,   0.,   0.,   0.,   0.,  26., 170., 226.,\n",
       "       209.,  97.,  38.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0., 129., 253., 165.,  16.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  67., 223., 254., 209.,  13.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 180., 228.,\n",
       "        52.,   3.,   0.,   0.,   0.,   0.,   0., 102., 235., 253., 128.,\n",
       "        22.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,  92., 254., 120.,   0.,   0.,   0.,   0.,\n",
       "        26., 205., 254., 245., 176.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 117.,\n",
       "       253., 225., 120.,  57., 132., 169., 244., 254., 215.,  81.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   7., 188., 253., 253., 254., 253.,\n",
       "       253., 215., 156.,  19.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  95., 229., 253., 255., 177.,  52.,  16.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[35999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "90UIitfWUp_L"
   },
   "source": [
    "### You can use the .reshape(28,28) function and plt.imshow() function with the parameters cmap = matplotlib.cm.binary, interpolation=\"nearest\" to make a plot of the number. Be sure to import matplotlib!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euR2jMOEUp_L"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1748d9e9488>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[35999].reshape(28, 28), cmap=plt.cm.binary, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lg12cNV4Up_O"
   },
   "source": [
    "### Use indexing to see if what the plot shows matches with the outcome of the 36000th index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IIrGU5pEUp_O"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[35999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "froaQc03Up_Q"
   },
   "source": [
    "### Now lets break into a test train split to run a classification. Instead of using sklearn, use indexing to select the first 60000 entries for the training, and the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VrOasZSCUp_Q"
   },
   "outputs": [],
   "source": [
    "X_train= X[:60000]\n",
    "X_test=  X[60000:]\n",
    "y_train= y[:60000]\n",
    "y_test=  y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5', '0', '4', ..., '5', '6', '8'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sdyH3KfzUp_T"
   },
   "source": [
    "### We are going to make a two-class classifier, so lets restrict to just one number, for example 5s. Do this by defining a new y training and y testing sets for just the number 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O23sEDJjUp_T"
   },
   "outputs": [],
   "source": [
    "y_train= y[:60000]\n",
    "y_test=  y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6vQQALZUp_V"
   },
   "source": [
    "### Lets train a logistic regression to predict if a number is a 5 or not (remember to use the 'just 5s' y training set!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlPavPMzUp_V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmXXgi5TUp_X"
   },
   "source": [
    "### Does the classifier predict correctly the 36000th digit we picked before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8MlPuH2SUp_Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALx4JWkFUp_a"
   },
   "source": [
    "### To make some comparisons, we are going to make a very dumb classifier, that never predicts 5s. Build the classifier with the code below, and call it using: never_5_clf = Never5Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7X7WdYMtUp_b"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "\n",
    "never_5_clf = Never5Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6b5bZIcXUp_d"
   },
   "source": [
    "### Now lets fit and predict on the testing set using our never 5 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YAP7A9LlUp_e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VrYZYL3aUp_g"
   },
   "source": [
    "### Let's compare this to the Logistic Regression. Examine the confusion matrix, precision, recall, and f1_scores for each. What is the probability cutoff you are using to decide the classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9aPqBXOUp_g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lvbi-m47Up_i"
   },
   "source": [
    "### What are the differences you see? Without knowing what each model is, what can these metrics tell you about how well each works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4JycFxNUp_i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szmx0qxBUp_k"
   },
   "source": [
    "### Now let's examine the roc_curve for each. Use the roc_curve method from sklearn.metrics to help plot the curve for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ObckzQRUp_k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPhGeQHuUp_m"
   },
   "source": [
    "### Now find the roc_auc_score for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vz4Gn4j_Up_o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pUfStJ6lUp_p"
   },
   "source": [
    "### Using the yellowbrick library  plot the roc_auc_score curve for the logistic model . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTytAzX7Up_q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vda4i6JZUp_s"
   },
   "source": [
    "### What does this metric tell you? Which classifier works better with this metric in mind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "94kkn4-hUp_s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
